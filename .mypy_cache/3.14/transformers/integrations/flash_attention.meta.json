{"data_mtime":1769764064,"dep_hashes":["873ce0c8cae541cb15f5c3e0f58e7c6c2ed37d41","8b3d4822a03fccfb70dcc21aaad2dbbdf7edbde5","9e5cba55701ed8e9e394e3a46914652a62c94696","55bdcba20555f7dc1e4993fab806e5623da9ed3b","f4436b9872f01355fc5cbc98ce01686084b09b94","52a795c21d6253ca6fc7f0eff61f891af1cc787f","82e850a550a5311633ba962d71eeb4afc90777c3","7a40c6ea5c303dfed22fc4a4b612d14521be014e","f089b290a55f47149fc59a74bd6cde26b060dd30","9b11b3af0bf77bf150f415d858a301f703dece1b","b31942015b5fd09a8b43bd6d0818c03b16ba8cd4"],"dep_lines":[4,3,4,1,1,1,1,1,1,1,1],"dep_prios":[10,5,20,10,5,30,30,30,30,30,30],"dependencies":["transformers.utils.logging","transformers.modeling_flash_attention_utils","transformers.utils","torch","builtins","abc","logging","torch._C","torch._tensor","torch.nn.modules.module","typing"],"error_lines":[],"hash":"2adaa20ec44b140f5ee115b95303d72e4dc7046e","id":"transformers.integrations.flash_attention","ignore_all":true,"interface_hash":"27fd052fd3a68bfd92e41f267f99bd5ef17b399a","mtime":1769725021,"options":{"other_options":"5b86a6c9d00200393d142204e8b026d3ea9a2b42","platform":"darwin"},"path":"/Users/mikko/.claude/.venv/lib/python3.14/site-packages/transformers/integrations/flash_attention.py","plugin_data":null,"size":3473,"suppressed":[],"version_id":"1.19.1"}